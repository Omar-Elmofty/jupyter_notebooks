{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6e674fe-a43d-42cf-89cd-06f35ce7676d",
   "metadata": {},
   "source": [
    "# CNN using Pytorch on the astar dataset\n",
    "Goal is to use CNNs to solve the supervised learning problem for finding the correct class label for the astar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "111cffea-16eb-4708-8021-72b597765617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "303cbc18-f107-4069-abe9-2c0d3c7dca64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 7.1953 seconds\n"
     ]
    }
   ],
   "source": [
    "# First let's load the dataset\n",
    "from datasets.astar_dataset import make_astar_dataset\n",
    "import time\n",
    "\n",
    "\n",
    "# Set the grid size here\n",
    "n, m = 5, 5  # grid size for the problem\n",
    "N = 100000     # Number of examples\n",
    "\n",
    "# Probability of existence of obstacle\n",
    "obstacle_probability = 0.3\n",
    "\n",
    "\n",
    "# Create the data set\n",
    "start = time.time()\n",
    "X, y = make_astar_dataset(N, n, m, obstacle_probability)\n",
    "print(f\"Execution time: {time.time() - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4374d02-80ab-4aaf-ad3f-2635658b4872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.00000</td>\n",
       "      <td>100000.00000</td>\n",
       "      <td>100000.00000</td>\n",
       "      <td>100000.00000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.19030</td>\n",
       "      <td>0.19030</td>\n",
       "      <td>0.19030</td>\n",
       "      <td>0.19030</td>\n",
       "      <td>0.238800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.39254</td>\n",
       "      <td>0.39254</td>\n",
       "      <td>0.39254</td>\n",
       "      <td>0.39254</td>\n",
       "      <td>0.426352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0             1             2             3              4\n",
       "count  100000.00000  100000.00000  100000.00000  100000.00000  100000.000000\n",
       "mean        0.19030       0.19030       0.19030       0.19030       0.238800\n",
       "std         0.39254       0.39254       0.39254       0.39254       0.426352\n",
       "min         0.00000       0.00000       0.00000       0.00000       0.000000\n",
       "25%         0.00000       0.00000       0.00000       0.00000       0.000000\n",
       "50%         0.00000       0.00000       0.00000       0.00000       0.000000\n",
       "75%         0.00000       0.00000       0.00000       0.00000       0.000000\n",
       "max         1.00000       1.00000       1.00000       1.00000       1.000000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Print the statistics of each class label\n",
    "y_df = pd.DataFrame(y)  # Convert y to a pandas Series if it's not already\n",
    "y_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e38a3636-aa39-44b4-8297-272a36ce807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels from one_hot vector to integer indicies\n",
    "import numpy as np\n",
    "y = np.argmax(y, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0317174-a266-4dac-a212-3d56c9bf8769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom torch dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "X_tensor = torch.from_numpy(X).float()\n",
    "y_tensor = torch.from_numpy(y).long()\n",
    "\n",
    "# Create a custom dataset class to retrieve the data\n",
    "class BinaryImageDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# Create the dataset and dataloader classes\n",
    "\n",
    "dataset = BinaryImageDataset(X_tensor, y_tensor)\n",
    "trainloader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf1f4c53-92ff-4c2d-8a50-e824c796366e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 5])\n",
      "right\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize some of the images\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "# Visualizing the dataset\n",
    "\n",
    "classes = (\"up\", \"left\", \"down\", \"right\", \"no_path\")\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "img = images[0]\n",
    "label = labels[0].item()\n",
    "\n",
    "print(img.shape)\n",
    "# imshow(torchvision.utils.make_grid(images))\n",
    "imshow(img)\n",
    "\n",
    "print(classes[label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a70ef2d1-7922-4ff2-bc3f-7dbb96aa9b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0527,  0.0615,  0.0454, -0.0872, -0.0777, -0.0027, -0.0445, -0.0235,\n",
       "         -0.0783,  0.0866]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now create a network that can solve this problem\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AstarNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AstarNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 120, 2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(120, 480, 2)\n",
    "        # self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(480, 240)\n",
    "        self.fc2 = nn.Linear(240, 120)\n",
    "        self.fc3 = nn.Linear(120, 84)\n",
    "        self.fc4 = nn.Linear(84, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        x = x.view(-1, 480)\n",
    "        # x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "net = AstarNetwork()\n",
    "net(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc8e12c-7364-4ca4-8622-3589774b1f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1999] loss: 0.22727266734711427\n",
      "[0, 3999] loss: 0.213988576487778\n",
      "[0, 5999] loss: 0.19804614429226222\n",
      "[0, 7999] loss: 0.19061898013843104\n",
      "[0, 9999] loss: 0.18713152634808589\n",
      "[0, 11999] loss: 0.19350145253446738\n",
      "[0, 13999] loss: 0.20071015042351323\n",
      "[0, 15999] loss: 0.18514169068333536\n",
      "[0, 17999] loss: 0.18736079027940175\n",
      "[0, 19999] loss: 0.19299359300277139\n",
      "[0, 21999] loss: 0.18784111979674942\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "# Define the loss and the optimizer\n",
    "criterion = nn.CrossEntropyLoss() # The cross entropy loss for classification\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "\n",
    "# Train\n",
    "for epoch in range(10): # Loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        # Zero gradient here is an important step\n",
    "        # Gradients are accumulated over a batch, if we don't reset the gradient here we will keep accumulating gradients\n",
    "        # making it impossible to train\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward() # Compute the gradients that will direct the learning\n",
    "        optimizer.step() # actually learn (one learning step)\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999: # print every 2000 minibatches\n",
    "            print(f\"[{epoch}, {i}] loss: {running_loss / 2000}\")\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bf1d8d5-4189-477c-9c56-b684ff0220cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "# There are 2 ways of saving the mode, either fully serialize it using pickle, or by saving just the params in the state dictionary\n",
    "\n",
    "torch.save(net.state_dict(), \"models/cnn_astar.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30cf63c3-db83-4187-9b8b-d874f8df7f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 89.856\n"
     ]
    }
   ],
   "source": [
    "# Training accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in trainloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Training accuracy: {100 * correct / total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3427141b-bd72-4aff-89df-4a80f124bdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 7.4611 seconds\n"
     ]
    }
   ],
   "source": [
    "# Generate a test set and test on it\n",
    "# Create the data set\n",
    "\n",
    "N_test = 100000\n",
    "\n",
    "start = time.time()\n",
    "X_test, y_test = make_astar_dataset(N_test, n, m, obstacle_probability)\n",
    "print(f\"Execution time: {time.time() - start:.4f} seconds\")\n",
    "\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "\n",
    "test_dataset = BinaryImageDataset(X_test, y_test)\n",
    "testloader = DataLoader(test_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c43e3f8-b9b9-4440-b052-3074d17089c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 88.328\n"
     ]
    }
   ],
   "source": [
    "# Testing accuracy\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Testing accuracy: {100 * correct / total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb1421e-9de9-4e42-81f5-a7b38d77a6db",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- Test with different model architectures\n",
    "- Explore imitation learning techniques\n",
    "- Explore RL techniques\n",
    "Good article summarizing the different approaches that exist https://smartlabai.medium.com/a-brief-overview-of-imitation-learning-8a8a75c44a9c\n",
    "- Test with imitation learning https://imitation.readthedocs.io/en/latest/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
